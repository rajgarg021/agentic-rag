{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJXW_DgiSebM"
      },
      "source": [
        "# Agentic RAG Powered by LangChain\n",
        "\n",
        "Building Agentic RAG Applications with LangChain.\n",
        "\n",
        "We'll be relying on a few great tools to help us do this:\n",
        "\n",
        "1. LangChain - more specifically LCEL\n",
        "2. LangGraph\n",
        "3. LangSmith\n",
        "\n",
        "Let's get started with a quick overview of LCEL and build a simple RAG chain!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_fLDElOVoop"
      },
      "source": [
        "## Dependencies\n",
        "\n",
        "We'll first install all our required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaVwN269EttM",
        "outputId": "4934e6e0-9b89-43e1-a11e-0556edff9b98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/49.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain langchain_openai langchain-community langgraph arxiv duckduckgo-search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBvTZ-JshKuo",
        "outputId": "40ed9668-7509-430b-f161-b45dead26e6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m96.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m92.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU faiss-cpu pymupdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wujPjGJuoPwg"
      },
      "source": [
        "## Environment Variables\n",
        "\n",
        "We'll want to set both our OpenAI API key and our LangSmith environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jdh8CoVWHRvs",
        "outputId": "a07594fc-a278-4170-9e0d-97782158bd2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API Key:··········\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv0glIDyHmRt",
        "outputId": "a800dd89-62ed-428d-e02d-03096a28478d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangSmith API Key: ··········\n"
          ]
        }
      ],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE1 - LangGraph - {uuid4().hex[0:8]}\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangSmith API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-CufnKBfFMq"
      },
      "source": [
        "## Initialize a Simple Chain using LCEL\n",
        "\n",
        "Familiarizing with LCEL and the specific ins and outs of how we can use it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnuLQnqzg0DZ"
      },
      "source": [
        "### Retrieval\n",
        "\n",
        "First, we'll set up a simple local retriever system that looks at Arxiv papers on the topic of RAG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8rnnJbJg8sB"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import ArxivLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "docs = ArxivLoader(query=\"Retrieval Augmented Generation\", load_max_docs=5).load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=350, chunk_overlap=50\n",
        ")\n",
        "\n",
        "chunked_documents = text_splitter.split_documents(docs)\n",
        "\n",
        "faiss_vectorstore = FAISS.from_documents(\n",
        "    documents=chunked_documents,\n",
        "    embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"),\n",
        ")\n",
        "\n",
        "retriever = faiss_vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-rSWYKjinCA"
      },
      "source": [
        "### Augmented\n",
        "\n",
        "Now that we have our retrieval system ready to rock, we can create our RAG prompt!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZudebB0it20"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_PROMPT = \"\"\"\\\n",
        "Use the following context to answer the user's query. If you cannot answer the question, please respond with 'I don't know'.\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPVEVKmPfoYs"
      },
      "source": [
        "### Generation\n",
        "\n",
        "Let's start by initializing our model. In this case, we'll be using OpenAI's `gpt-3.5-turbo` model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAU6ilyPfufs"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "openai_chat_model = ChatOpenAI(model=\"gpt-3.5-turbo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhxJkDIujhC-"
      },
      "source": [
        "### LCEL RAG Chain\n",
        "\n",
        "Now that we have our R, A, and G components - let's build our simple RAG chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4itzKufj-OI"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "retrieval_augmented_generation_chain = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
        "    # \"question\" : populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
        "    {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
        "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
        "    #              by getting the value of the \"context\" key from the previous step\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #              into the LLM and stored in a key called \"response\"\n",
        "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
        "    | {\"response\": rag_prompt | openai_chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4vr1n-7kN-y"
      },
      "source": [
        "And let's test it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocoqybsjkPXz",
        "outputId": "6bd81c82-fde5-445e-d5b8-03635393872f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'response': AIMessage(content='Retrieval Augmented Generation is a text generation paradigm that fuses deep learning technology with traditional retrieval technology. It has achieved state-of-the-art performance in many natural language processing tasks and has gained attention in the computational linguistics community.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 47, 'prompt_tokens': 2186, 'total_tokens': 2233, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-b7465281-466c-46b7-9955-65226a350239-0', usage_metadata={'input_tokens': 2186, 'output_tokens': 47, 'total_tokens': 2233}),\n",
              " 'context': [Document(metadata={'Published': '2022-02-13', 'Title': 'A Survey on Retrieval-Augmented Text Generation', 'Authors': 'Huayang Li, Yixuan Su, Deng Cai, Yan Wang, Lemao Liu', 'Summary': 'Recently, retrieval-augmented text generation attracted increasing attention\\nof the computational linguistics community. Compared with conventional\\ngeneration models, retrieval-augmented text generation has remarkable\\nadvantages and particularly has achieved state-of-the-art performance in many\\nNLP tasks. This paper aims to conduct a survey about retrieval-augmented text\\ngeneration. It firstly highlights the generic paradigm of retrieval-augmented\\ngeneration, and then it reviews notable approaches according to different tasks\\nincluding dialogue response generation, machine translation, and other\\ngeneration tasks. Finally, it points out some important directions on top of\\nrecent methods to facilitate future research.'}, page_content='grating translation memory to NMT models (Gu\\net al., 2018; Zhang et al., 2018; Xu et al., 2020;\\nHe et al., 2021). We also review the applications\\nof retrieval-augmented generation in other genera-\\ntion tasks such as abstractive summarization (Peng\\net al., 2019), code generation (Hashimoto et al.,\\n2018), paraphrase (Kazemnejad et al., 2020; Su\\net al., 2021b), and knowledge-intensive generation\\n(Lewis et al., 2020b). Finally, we also point out\\nsome promising directions on retrieval-augmented\\ngeneration to push forward the future research.\\n2\\nRetrieval-Augmented Paradigm\\nIn this section, we ﬁrst give a general formulation\\nof retrieval-augmented text generation. Then, we\\ndiscuss three major components of the retrieval-\\naugmented generation paradigm, including the re-\\narXiv:2202.01110v2  [cs.CL]  13 Feb 2022\\nInput\\nSources \\n(Sec. 2.2):\\nTraining \\nCorpus\\nExternal Data\\nUnsupervised \\nData\\nMetrics\\n(Sec. 2.3):\\nSparse-vector \\nRetrieval\\nDense-vector \\nRetrieval\\nTask-specific \\nRetrieval\\nRetrieval Memory\\nGeneration Model\\nSec. 4: Machine \\nTranslation\\nSec. 5: Other \\nTasks\\nData \\nAugmentation\\nAttention \\nMechanism\\nSkeleton & \\nTemplates\\nInformation Retrieval'),\n",
              "  Document(metadata={'Published': '2022-02-13', 'Title': 'A Survey on Retrieval-Augmented Text Generation', 'Authors': 'Huayang Li, Yixuan Su, Deng Cai, Yan Wang, Lemao Liu', 'Summary': 'Recently, retrieval-augmented text generation attracted increasing attention\\nof the computational linguistics community. Compared with conventional\\ngeneration models, retrieval-augmented text generation has remarkable\\nadvantages and particularly has achieved state-of-the-art performance in many\\nNLP tasks. This paper aims to conduct a survey about retrieval-augmented text\\ngeneration. It firstly highlights the generic paradigm of retrieval-augmented\\ngeneration, and then it reviews notable approaches according to different tasks\\nincluding dialogue response generation, machine translation, and other\\ngeneration tasks. Finally, it points out some important directions on top of\\nrecent methods to facilitate future research.'}, page_content='augmented generation as well as three key com-\\nponents under this paradigm, which are retrieval\\nsources, retrieval metrics and generation models.\\nThen, we introduce notable methods about\\nretrieval-augmented generation, which are orga-\\nnized with respect to different tasks. Speciﬁcally,\\non the dialogue response generation task, exem-\\nplar/template retrieval as an intermediate step has\\nbeen shown beneﬁcial to informative response gen-\\neration (Weston et al., 2018; Wu et al., 2019; Cai\\net al., 2019a,b). In addition, there has been growing\\ninterest in knowledge-grounded generation explor-\\ning different forms of knowledge such as knowl-\\nedge bases and external documents (Dinan et al.,\\n2018; Zhou et al., 2018; Lian et al., 2019; Li et al.,\\n2019; Qin et al., 2019; Wu et al., 2021; Zhang et al.,\\n2021). On the machine translation task, we summa-\\nrize the early work on how the retrieved sentences\\n(called translation memory) are used to improve\\nstatistical machine translation (SMT) (Koehn et al.,\\n2003) models (Simard and Isabelle, 2009; Koehn\\nand Senellart, 2010) and in particular, we inten-\\nsively highlight several popular methods to inte-\\ngrating translation memory to NMT models (Gu\\net al., 2018; Zhang et al., 2018; Xu et al., 2020;\\nHe et al., 2021). We also review the applications'),\n",
              "  Document(metadata={'Published': '2022-02-13', 'Title': 'A Survey on Retrieval-Augmented Text Generation', 'Authors': 'Huayang Li, Yixuan Su, Deng Cai, Yan Wang, Lemao Liu', 'Summary': 'Recently, retrieval-augmented text generation attracted increasing attention\\nof the computational linguistics community. Compared with conventional\\ngeneration models, retrieval-augmented text generation has remarkable\\nadvantages and particularly has achieved state-of-the-art performance in many\\nNLP tasks. This paper aims to conduct a survey about retrieval-augmented text\\ngeneration. It firstly highlights the generic paradigm of retrieval-augmented\\ngeneration, and then it reviews notable approaches according to different tasks\\nincluding dialogue response generation, machine translation, and other\\ngeneration tasks. Finally, it points out some important directions on top of\\nrecent methods to facilitate future research.'}, page_content='recent methods to facilitate future research.\\n1\\nIntroduction\\nRetrieval-augmented text generation, as a new\\ntext generation paradigm that fuses emerging deep\\nlearning technology and traditional retrieval tech-\\nnology, has achieved state-of-the-art (SOTA) per-\\nformance in many NLP tasks and attracted the at-\\ntention of the computational linguistics community\\n(Weston et al., 2018; Dinan et al., 2018; Cai et al.,\\n2021). Compared with generation-based counter-\\npart, this new paradigm has some remarkable ad-\\nvantages: 1) The knowledge is not necessary to be\\nimplicitly stored in model parameters, but is explic-\\nitly acquired in a plug-and-play manner, leading\\nto great scalibility; 2) Instead of generating from\\nscratch, the paradigm generating text from some re-\\ntrieved human-written reference, which potentially\\nalleviates the difﬁculty of text generation.\\nThis paper aims to review many representative\\napproaches for retrieval-augmented text generation\\ntasks including dialogue response generation (We-\\nston et al., 2018), machine translation (Gu et al.,\\n2018) and others (Hashimoto et al., 2018). We\\n∗All authors contributed equally.\\nﬁrstly present the generic paradigm of retrieval-\\naugmented generation as well as three key com-\\nponents under this paradigm, which are retrieval\\nsources, retrieval metrics and generation models.\\nThen, we introduce notable methods about'),\n",
              "  Document(metadata={'Published': '2022-02-13', 'Title': 'A Survey on Retrieval-Augmented Text Generation', 'Authors': 'Huayang Li, Yixuan Su, Deng Cai, Yan Wang, Lemao Liu', 'Summary': 'Recently, retrieval-augmented text generation attracted increasing attention\\nof the computational linguistics community. Compared with conventional\\ngeneration models, retrieval-augmented text generation has remarkable\\nadvantages and particularly has achieved state-of-the-art performance in many\\nNLP tasks. This paper aims to conduct a survey about retrieval-augmented text\\ngeneration. It firstly highlights the generic paradigm of retrieval-augmented\\ngeneration, and then it reviews notable approaches according to different tasks\\nincluding dialogue response generation, machine translation, and other\\ngeneration tasks. Finally, it points out some important directions on top of\\nrecent methods to facilitate future research.'}, page_content='A Survey on Retrieval-Augmented Text Generation\\nHuayang Li♥,∗\\nYixuan Su♠,∗\\nDeng Cai♦,∗\\nYan Wang♣,∗\\nLemao Liu♣,∗\\n♥Nara Institute of Science and Technology\\n♠University of Cambridge\\n♦The Chinese University of Hong Kong\\n♣Tencent AI Lab\\nli.huayang.lh6@is.naist.jp, ys484@cam.ac.uk\\nthisisjcykcd@gmail.com, brandenwang@tencent.com\\nlemaoliu@gmail.com\\nAbstract\\nRecently, retrieval-augmented text generation\\nattracted increasing attention of the compu-\\ntational linguistics community.\\nCompared\\nwith conventional generation models, retrieval-\\naugmented text generation has remarkable ad-\\nvantages and particularly has achieved state-of-\\nthe-art performance in many NLP tasks. This\\npaper aims to conduct a survey about retrieval-\\naugmented text generation. It ﬁrstly highlights\\nthe generic paradigm of retrieval-augmented\\ngeneration, and then it reviews notable ap-\\nproaches according to different tasks including\\ndialogue response generation, machine trans-\\nlation, and other generation tasks. Finally, it\\npoints out some promising directions on top of\\nrecent methods to facilitate future research.\\n1\\nIntroduction\\nRetrieval-augmented text generation, as a new\\ntext generation paradigm that fuses emerging deep\\nlearning technology and traditional retrieval tech-')]}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "await retrieval_augmented_generation_chain.ainvoke({\"question\" : \"What is Retrieval Augmented Generation?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7pQDUhUnIo8"
      },
      "source": [
        "## LangGraph - Building Cyclic Applications with LangChain\n",
        "\n",
        "LangGraph is a tool that leverages LangChain Expression Language to build coordinated multi-actor and stateful applications that includes cyclic behaviour.\n",
        "\n",
        "### Why Cycles?\n",
        "\n",
        "In essence, we can think of a cycle in our graph as a more robust and customizable loop. It allows us to keep our application agent-forward while still giving the powerful functionality of traditional loops.\n",
        "\n",
        "Due to the inclusion of cycles over loops, we can also compose rather complex flows through our graph in a much more readable and natural fashion. Effetively allowing us to recreate appliation flowcharts in code in an almost 1-to-1 fashion.\n",
        "\n",
        "### Why LangGraph?\n",
        "\n",
        "Beyond the agent-forward approach - we can easily compose and combine traditional \"DAG\" (directed acyclic graph) chains with powerful cyclic behaviour due to the tight integration with LCEL. This means it's a natural extension to LangChain's core offerings!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBRyQmEAVzua"
      },
      "source": [
        "##  Creating our Tool Belt\n",
        "\n",
        "As is usually the case, we'll want to equip our agent with a toolbelt to help answer questions and add external knowledge.\n",
        "\n",
        "There's a tonne of tools in the [LangChain Community Repo](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools) but we'll stick to a couple just so we can observe the cyclic nature of LangGraph in action!\n",
        "\n",
        "We'll leverage:\n",
        "\n",
        "- [Duck Duck Go Web Search](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/ddg_search)\n",
        "- [Arxiv](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/arxiv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAxaSvlfIeOg"
      },
      "outputs": [],
      "source": [
        "from langchain_community.tools.ddg_search import DuckDuckGoSearchRun\n",
        "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
        "\n",
        "tool_belt = [\n",
        "    DuckDuckGoSearchRun(),\n",
        "    ArxivQueryRun()\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FdOjEslXdRR"
      },
      "source": [
        "### Actioning with Tools\n",
        "\n",
        "Now that we've created our tool belt - we need to create a process that will let us leverage them when we need them.\n",
        "\n",
        "We'll use the built-in [`ToolExecutor`](https://github.com/langchain-ai/langgraph/blob/main/langgraph/prebuilt/tool_executor.py) to do so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFr1m80-JZsD",
        "outputId": "4a3bc27d-37ff-48b7-824e-3e4656cbcc73"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-8253c437cb34>:3: LangGraphDeprecationWarning: ToolExecutor is deprecated as of version 0.2.0 and will be removed in 0.3.0. Use langgraph.prebuilt.ToolNode instead.\n",
            "  tool_executor = ToolExecutor(tool_belt)\n"
          ]
        }
      ],
      "source": [
        "from langgraph.prebuilt import ToolExecutor\n",
        "\n",
        "tool_executor = ToolExecutor(tool_belt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI-C669ZYVI5"
      },
      "source": [
        "### Model\n",
        "\n",
        "Now we can set-up our model! We'll leverage the familiar OpenAI model suite for this example - but it's not *necessary* to use with LangGraph. LangGraph supports all models - though you might not find success with smaller models - as such, they recommend you stick with:\n",
        "\n",
        "- OpenAI's GPT-3.5 and GPT-4\n",
        "- Anthropic's Claude\n",
        "- Google's Gemini\n",
        "\n",
        "> NOTE: Because we're leveraging the OpenAI function calling API - we'll need to use OpenAI *for this specific example* (or any other service that exposes an OpenAI-style function calling API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkNS8rNZJs4z"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugkj3GzuZpQv"
      },
      "source": [
        "Now that we have our model set-up, let's \"put on the tool belt\", which is to say: We'll bind our LangChain formatted tools to the model in an OpenAI function calling format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OdMqFafZ_0V"
      },
      "outputs": [],
      "source": [
        "from langchain_core.utils.function_calling import convert_to_openai_function\n",
        "\n",
        "functions = [convert_to_openai_function(t) for t in tool_belt]\n",
        "model = model.bind_functions(functions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_296Ub96Z_H8"
      },
      "source": [
        "## Putting the State in Stateful\n",
        "\n",
        "So what does that \"stateful\" mean?\n",
        "\n",
        "To put it simply - we want to have some kind of object which we can pass around our application that holds information about what the current situation (state) is. Since our system will be constructed of many parts moving in a coordinated fashion - we want to be able to ensure we have some commonly understood idea of that state.\n",
        "\n",
        "LangGraph leverages a `StatefulGraph` which uses an `AgentState` object to pass information between the various nodes of the graph.\n",
        "\n",
        "There are more options than what we'll see below - but this `AgentState` object is one that is stored in a `TypedDict` with the key `messages` and the value is a `Sequence` of `BaseMessages` that will be appended to whenever the state changes.\n",
        "\n",
        "Let's think about a simple example to help understand exactly what this means (we'll simplify a great deal to try and clearly communicate what state is doing):\n",
        "\n",
        "1. We initialize our state object:\n",
        "  - `{\"messages\" : []}`\n",
        "2. Our user submits a query to our application.\n",
        "  - New State: `HumanMessage(#1)`\n",
        "  - `{\"messages\" : [HumanMessage(#1)}`\n",
        "3. We pass our state object to an Agent node which is able to read the current state. It will use the last `HumanMessage` as input. It gets some kind of output which it will add to the state.\n",
        "  - New State: `AgentMessage(#1, additional_kwargs {\"function_call\" : \"WebSearchTool\"})`\n",
        "  - `{\"messages\" : [HumanMessage(#1), AgentMessage(#1, ...)]}`\n",
        "4. We pass our state object to a \"conditional node\" (more on this later) which reads the last state to determine if we need to use a tool - which it can determine properly because of our provided object!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxL9b_NZKUdL"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated, Sequence\n",
        "import operator\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[Sequence[BaseMessage], operator.add]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWsMhfO9grLu"
      },
      "source": [
        "## It's Graphing Time!\n",
        "\n",
        "Now that we have state, and we have tools, and we have an LLM - we can finally start making our graph!\n",
        "\n",
        "Let's take a second to refresh ourselves about what a graph is in this context.\n",
        "\n",
        "Graphs, also called networks in some circles, are a collection of connected objects.\n",
        "\n",
        "The objects in question are typically called nodes, or vertices, and the connections are called edges.\n",
        "\n",
        "Let's look at a simple graph.\n",
        "\n",
        "![image](https://i.imgur.com/2NFLnIc.png)\n",
        "\n",
        "Here, we're using the coloured circles to represent the nodes and the yellow lines to represent the edges. In this case, we're looking at a fully connected graph - where each node is connected by an edge to each other node.\n",
        "\n",
        "If we were to think about nodes in the context of LangGraph - we would think of a function, or an LCEL runnable.\n",
        "\n",
        "If we were to think about edges in the context of LangGraph - we might think of them as \"paths to take\" or \"where to pass our state object next\".\n",
        "\n",
        "Let's create some nodes and expand on our diagram.\n",
        "\n",
        "> NOTE: Due to the tight integration with LCEL - we can comfortably create our nodes in an async fashion!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91flJWtZLUrl"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolInvocation\n",
        "import json\n",
        "from langchain_core.messages import FunctionMessage\n",
        "\n",
        "def call_model(state):\n",
        "  messages = state[\"messages\"]\n",
        "  response = model.invoke(messages)\n",
        "  return {\"messages\" : [response]}\n",
        "\n",
        "def call_tool(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  action = ToolInvocation(\n",
        "      tool=last_message.additional_kwargs[\"function_call\"][\"name\"],\n",
        "      tool_input=json.loads(\n",
        "          last_message.additional_kwargs[\"function_call\"][\"arguments\"]\n",
        "      )\n",
        "  )\n",
        "\n",
        "  response = tool_executor.invoke(action)\n",
        "\n",
        "  function_message = FunctionMessage(content=str(response), name=action.tool)\n",
        "\n",
        "  return {\"messages\" : [function_message]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bwR7MgWj3Wg"
      },
      "source": [
        "Now we have two total nodes. We have:\n",
        "\n",
        "- `call_model` is a node that will...well...call the model\n",
        "- `call_tool` is a node which will call a tool\n",
        "\n",
        "Let's start adding nodes! We'll update our diagram along the way to keep track of what this looks like!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vF4_lgtmQNo"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "workflow.add_node(\"agent\", call_model)\n",
        "workflow.add_node(\"action\", call_tool)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8CjRlbVmRpW"
      },
      "source": [
        "Let's look at what we have so far:\n",
        "\n",
        "![image](https://i.imgur.com/md7inqG.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaXHpPeSnOWC"
      },
      "source": [
        "Next, we'll add our entrypoint. All our entrypoint does is indicate which node is called first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGCbaYqRnmiw"
      },
      "outputs": [],
      "source": [
        "workflow.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUsfGoSpoF9U"
      },
      "source": [
        "![image](https://i.imgur.com/wNixpJe.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q_pQgHmoW0M"
      },
      "source": [
        "Now we want to build a \"conditional edge\" which will use the output state of a node to determine which path to follow.\n",
        "\n",
        "We can help conceptualize this by thinking of our conditional edge as a conditional in a flowchart!\n",
        "\n",
        "Notice how our function simply checks if there is a \"function_call\" kwarg present.\n",
        "\n",
        "Then we create an edge where the origin node is our agent node and our destination node is *either* the action node or the END (finish the graph).\n",
        "\n",
        "It's important to highlight that the dictionary passed in as the third parameter (the mapping) should be created with the possible outputs of our conditional function in mind. In this case `should_continue` outputs either `\"end\"` or `\"continue\"` which are subsequently mapped to the action node or the END node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BZgb81VQf9o"
      },
      "outputs": [],
      "source": [
        "def should_continue(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  if \"function_call\" not in last_message.additional_kwargs:\n",
        "    return \"end\"\n",
        "\n",
        "  return \"continue\"\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"continue\" : \"action\",\n",
        "        \"end\" : END\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Cvhcf4jp0Ce"
      },
      "source": [
        "Let's visualize what this looks like.\n",
        "\n",
        "![image](https://i.imgur.com/8ZNwKI5.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKCjWJCkrJb9"
      },
      "source": [
        "Finally, we can add our last edge which will connect our action node to our agent node. This is because we *always* want our action node (which is used to call our tools) to return its output to our agent!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvcgbHf1rIXZ"
      },
      "outputs": [],
      "source": [
        "workflow.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiWDwBQtrw7Z"
      },
      "source": [
        "Let's look at the final visualization.\n",
        "\n",
        "![image](https://i.imgur.com/NWO7usO.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYqDpErlsCsu"
      },
      "source": [
        "All that's left to do now is to compile our workflow - and we're off!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zt9-KS8DpzNx"
      },
      "outputs": [],
      "source": [
        "app = workflow.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEYcTShCsPaa"
      },
      "source": [
        "## Using Our Graph\n",
        "\n",
        "Now that we've created and compiled our graph - we can call it *just as we'd call any other* `Runnable`!\n",
        "\n",
        "Let's try out a few examples to see how it fairs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn4n37PQRPII",
        "outputId": "8423a464-fb56-43bc-82ff-cda9fa46edd0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='What is RAG in the context of Large Language Models? When did it break onto the scene?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='RAG stands for Retrieval-Augmented Generation. It is a technique used in Large Language Models to improve the generation of text by combining retrieval-based methods with generation-based methods. RAG models use a retriever to search for relevant information from a large corpus of text and then use a generator to generate text based on the retrieved information.\\n\\nRAG broke onto the scene in 2020 when a research paper titled \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\" was published by researchers from Facebook AI. This paper introduced the RAG framework and demonstrated its effectiveness in various natural language processing tasks. Since then, RAG has gained popularity in the field of Large Language Models for its ability to generate more informative and coherent text by leveraging external knowledge sources.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 156, 'prompt_tokens': 171, 'total_tokens': 327, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-8684f13e-6a55-49c9-a504-dda3c500e96b-0', usage_metadata={'input_tokens': 171, 'output_tokens': 156, 'total_tokens': 327})]}"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "inputs = {\"messages\" : [HumanMessage(content=\"What is RAG in the context of Large Language Models? When did it break onto the scene?\")]}\n",
        "\n",
        "app.invoke(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBHnUtLSscRr"
      },
      "source": [
        "Let's look at what happened:\n",
        "\n",
        "1. Our state object was populated with our request\n",
        "2. The state object was passed into our entry point (agent node) and the agent node added an `AIMessage` to the state object and passed it along the conditional edge\n",
        "3. The conditional edge received the state object, found the \"function_call\" `additional_kwarg`, and sent the state object to the action node\n",
        "4. The action node added the response from the OpenAI function calling endpoint to the state object and passed it along the edge to the agent node\n",
        "5. The agent node added a response to the state object and passed it along the conditional edge\n",
        "6. The conditional edge received the state object, could not find the \"function_call\" `additional_kwarg` and passed the state object to END where we see it output in the cell above!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXGeN-C8mbBM"
      },
      "source": [
        "## Agentic RAG with LangGraph and LCEL\n",
        "\n",
        "Let's see what our final graph will look like:\n",
        "\n",
        "![image](https://i.imgur.com/aL4Qs0E.png)\n",
        "\n",
        "Now that we have our two major components, let's create our final agent!\n",
        "\n",
        "Since we can add any LCEL `Runnable` to our graph as a node directly - we can add our RAG chain as a node with no additional steps!\n",
        "\n",
        "However, since our `Runnable` was set-up without knowledge of our state object - we need to add some pre/post processing steps to ensure it fits into the flow!\n",
        "\n",
        "> NOTE: There is only one cycle in this graph, as we cannot reach the RAG chain after the initial attempt to use it.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlCwP-iXojru"
      },
      "outputs": [],
      "source": [
        "def convert_state_to_query(state_object):\n",
        "  return {\"question\" : state_object[\"messages\"][-1].content}\n",
        "\n",
        "def convert_response_to_state(response):\n",
        "  return {\"messages\" : [response[\"response\"]]}\n",
        "\n",
        "langgraph_node_rag_chain = convert_state_to_query | retrieval_augmented_generation_chain | convert_response_to_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTXZtCgouJ2z"
      },
      "source": [
        "Let's test our our new chain and verify it works as expected!\n",
        "\n",
        "> NOTE: We are still able to take advantage of the benefits of built in `async` provided by LCEL with this chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ku8Vk0krNU_",
        "outputId": "00bcc2ba-395b-4c2d-c0d4-9d380920eeed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'messages': [AIMessage(content=\"RAG in the context of Large Language Models stands for Retrieval Augmented Generation. It has been applied in various scenarios to enhance LLMs with external documents provided by retrievers. The RAG framework involves retrievers locating and retrieving useful documents based on a given query, which are then used by LLMs to generate a response. The R^2AG, a novel enhanced RAG framework, was introduced to bridge the semantic gap between LLMs and retrievers by incorporating Retrieval information into Retrieval Augmented Generation. It suits low-source scenarios where LLMs and retrievers are frozen. The R^2AG framework utilizes nuanced features from the retrievers and employs a R^2-Former to capture retrieval information, along with a retrieval-aware prompting strategy to integrate retrieval information into LLMs' generation processes.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 167, 'prompt_tokens': 2704, 'total_tokens': 2871, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-d7ddf670-48f6-45bb-a23a-981148d33ac8-0', usage_metadata={'input_tokens': 2704, 'output_tokens': 167, 'total_tokens': 2871})]}"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "await langgraph_node_rag_chain.ainvoke(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqz0HD_-uVLq"
      },
      "source": [
        "Now we'll add our nodes - notice that we're including our newly built LCEL component as a node called `first_action`.\n",
        "\n",
        "The basic idea is that we will use our private RAG set-up - and if that is deemed sufficient, we will return that response to our user; and if not we will augment our response will the other tools!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvMuxumArtT7"
      },
      "outputs": [],
      "source": [
        "rag_agent = StateGraph(AgentState)\n",
        "\n",
        "rag_agent.add_node(\"agent\", call_model)\n",
        "rag_agent.add_node(\"action\", call_tool)\n",
        "rag_agent.add_node(\"first_action\", langgraph_node_rag_chain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaxHeBtsz-JF"
      },
      "source": [
        "Let's set our new entry point to be our RAG pipeline!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpHI3VhDr72t"
      },
      "outputs": [],
      "source": [
        "rag_agent.set_entry_point(\"first_action\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-GFjQen0BeH"
      },
      "source": [
        "Because we wish to have this conditional behaviour (\"is the question fully answered by the RAG pipeline?\") we'll need to add a new conditional node!\n",
        "\n",
        "We'll start by describing a process by which we can ask the question: \"Is this question fully answered by the response?\"\n",
        "\n",
        "This will let us boil down our paths to \"Yes, it is fully answered\", and \"No, it is not fully answered\".\n",
        "\n",
        "The function below will do exactly that by leaning on Pydantic and GPT-4!\n",
        "\n",
        "> NOTE: We now have an LCEL component as a node, and we have a chain *inside a function in as a node*. LangGraph is an extremely flexible framework!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-QZVNURvDTe",
        "outputId": "3b9588fa-83da-4800-a9bb-88b1159b1ebd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3553: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
            "\n",
            "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
            "with: `from pydantic import BaseModel`\n",
            "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
            "\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain.output_parsers.openai_tools import PydanticToolsParser\n",
        "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
        "\n",
        "def is_fully_answered(state):\n",
        "\n",
        "  ### Extract the question and response from our RAG pipeline\n",
        "  question = state[\"messages\"][0].content\n",
        "  answer = state[\"messages\"][-1].content\n",
        "\n",
        "  ### Create a Pydantic model to capture our LLMs response\n",
        "  class answered(BaseModel):\n",
        "    binary_score: str = Field(description=\"Fully answered: 'yes' or 'no'\")\n",
        "\n",
        "  ### A powerful reasoning model will ensure we can answer our question properly\n",
        "  model = ChatOpenAI(model=\"gpt-4-turbo-preview\", temperature=0)\n",
        "\n",
        "  ### Create and bind our tool to our model\n",
        "  answered_tool = convert_to_openai_tool(answered)\n",
        "\n",
        "  model = model.bind(\n",
        "      tools=[answered_tool],\n",
        "      tool_choice={\"type\" : \"function\", \"function\" : {\"name\" : \"answered\"}}\n",
        "  )\n",
        "\n",
        "  ### We'll want to parse the output into a usable format\n",
        "  parser_tool = PydanticToolsParser(tools=[answered])\n",
        "\n",
        "  prompt = PromptTemplate(\n",
        "      template=\"\"\"You will determine if the question is fully answered by the response.\\n\n",
        "      Question:\n",
        "      {question}\n",
        "\n",
        "      Response:\n",
        "      {answer}\n",
        "\n",
        "      You will respond with either 'yes' or 'no'.\"\"\",\n",
        "      input_variables=[\"question\", \"answer\"])\n",
        "\n",
        "  ### Classic LCEL chain!\n",
        "  fully_answered_chain = prompt | model | parser_tool\n",
        "\n",
        "  response = fully_answered_chain.invoke({\"question\" : question, \"answer\" : answer})\n",
        "\n",
        "  if response[0].binary_score == \"no\":\n",
        "    return \"continue\"\n",
        "\n",
        "  return \"end\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeG_LWxn1PJJ"
      },
      "source": [
        "Let's map and add that conditional edge now!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R83jwrx8xKVf"
      },
      "outputs": [],
      "source": [
        "rag_agent.add_conditional_edges(\n",
        "    \"first_action\",\n",
        "    is_fully_answered,\n",
        "    {\n",
        "        \"continue\" : \"agent\",\n",
        "        \"end\" : END\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFyVeahl1fdj"
      },
      "source": [
        "We'll still use our original prompt to determine if we need to use more tools or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WD8wC2W6r_tb"
      },
      "outputs": [],
      "source": [
        "def should_continue(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  if \"function_call\" not in last_message.additional_kwargs:\n",
        "    return \"end\"\n",
        "\n",
        "  return \"continue\"\n",
        "\n",
        "rag_agent.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"continue\" : \"action\",\n",
        "        \"end\" : END\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1BR2VMW1k7E"
      },
      "source": [
        "Let's define the final edge."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvQzFbtJsDrE"
      },
      "outputs": [],
      "source": [
        "rag_agent.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_8ZHvch1mqB"
      },
      "source": [
        "Time to compile!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bgho8n0QsO_s"
      },
      "outputs": [],
      "source": [
        "rag_agent_app = rag_agent.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lu069aIF1pAU"
      },
      "source": [
        "Let's try it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sItf-jgY1rMU",
        "outputId": "3c1f8a57-e01f-4288-d7db-0d94bd02b3c6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='Who is the main author on the Retrieval Augmented Generation paper?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Huayang Li is the main author on the Retrieval Augmented Generation paper.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 2196, 'total_tokens': 2213, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-58899443-c8c3-4fec-9afb-17a6d63d27d7-0', usage_metadata={'input_tokens': 2196, 'output_tokens': 17, 'total_tokens': 2213})]}"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question = \"Who is the main author on the Retrieval Augmented Generation paper?\"\n",
        "\n",
        "inputs = {\"messages\" : [HumanMessage(content=question)]}\n",
        "\n",
        "rag_agent_app.invoke(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHj-i2Cc1uud"
      },
      "source": [
        "Notice how we didn't enter into our tool cycle as the query was fully answered by our baseline RAG system!\n",
        "\n",
        "Let's try another example!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzQY4SSvsSE1",
        "outputId": "a94bc650-4977-47a2-eac3-ccea3314107e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-17-7fbae28d8eb0>:13: LangGraphDeprecationWarning: ToolInvocation is deprecated as of version 0.2.0 and will be removed in 0.3.0. Use langgraph.prebuilt.ToolNode instead.\n",
            "  action = ToolInvocation(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='Who is the main author on the Retrieval Augmented Generation paper - and how old is he?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"I don't know.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 2202, 'total_tokens': 2207, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-62731707-4c67-4a04-8dbb-b92d8bd46c89-0', usage_metadata={'input_tokens': 2202, 'output_tokens': 5, 'total_tokens': 2207}),\n",
              "  AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"query\":\"Retrieval Augmented Generation\"}', 'name': 'arxiv'}, 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 180, 'total_tokens': 199, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None}, id='run-121bc8b6-599e-4039-9219-fd11c4c69dda-0', usage_metadata={'input_tokens': 180, 'output_tokens': 19, 'total_tokens': 199}),\n",
              "  FunctionMessage(content=\"Published: 2024-06-19\\nTitle: R^2AG: Incorporating Retrieval Information into Retrieval Augmented Generation\\nAuthors: Fuda Ye, Shuangyin Li, Yongqi Zhang, Lei Chen\\nSummary: Retrieval augmented generation (RAG) has been applied in many scenarios to\\naugment large language models (LLMs) with external documents provided by\\nretrievers. However, a semantic gap exists between LLMs and retrievers due to\\ndifferences in their training objectives and architectures. This misalignment\\nforces LLMs to passively accept the documents provided by the retrievers,\\nleading to incomprehension in the generation process, where the LLMs are\\nburdened with the task of distinguishing these documents using their inherent\\nknowledge. This paper proposes R$^2$AG, a novel enhanced RAG framework to fill\\nthis gap by incorporating Retrieval information into Retrieval Augmented\\nGeneration. Specifically, R$^2$AG utilizes the nuanced features from the\\nretrievers and employs a R$^2$-Former to capture retrieval information. Then, a\\nretrieval-aware prompting strategy is designed to integrate retrieval\\ninformation into LLMs' generation. Notably, R$^2$AG suits low-source scenarios\\nwhere LLMs and retrievers are frozen. Extensive experiments across five\\ndatasets validate the effectiveness, robustness, and efficiency of R$^2$AG. Our\\nanalysis reveals that retrieval information serves as an anchor to aid LLMs in\\nthe generation process, thereby filling the semantic gap.\\n\\nPublished: 2022-02-13\\nTitle: A Survey on Retrieval-Augmented Text Generation\\nAuthors: Huayang Li, Yixuan Su, Deng Cai, Yan Wang, Lemao Liu\\nSummary: Recently, retrieval-augmented text generation attracted increasing attention\\nof the computational linguistics community. Compared with conventional\\ngeneration models, retrieval-augmented text generation has remarkable\\nadvantages and particularly has achieved state-of-the-art performance in many\\nNLP tasks. This paper aims to conduct a survey about retrieval-augmented text\\ngeneration. It firstly highlights the generic paradigm of retrieval-augmented\\ngeneration, and then it reviews notable approaches according to different tasks\\nincluding dialogue response generation, machine translation, and other\\ngeneration tasks. Finally, it points out some important directions on top of\\nrecent methods to facilitate future research.\\n\\nPublished: 2024-07-04\\nTitle: Meta-prompting Optimized Retrieval-augmented Generation\\nAuthors: João Rodrigues, António Branco\\nSummary: Retrieval-augmented generation resorts to content retrieved from external\\nsources in order to leverage the performance of large language models in\\ndownstream tasks. The excessive volume of retrieved content, the possible\\ndispersion of its parts, or their out of focus range may happen nevertheless to\\neventually have a detrimental rather than an incremental effect. To mitigate\\nthis issue and improve retrieval-augmented generation, we propose a method to\\nrefine the retrieved content before it is included in the prompt by resorting\\nto meta-prompting optimization. Put to empirical test with the demanding\\nmulti-hop question answering task from the StrategyQA dataset, the evaluation\\nresults indicate that this method outperforms a similar retrieval-augmented\\nsystem but without this method by over 30%.\", additional_kwargs={}, response_metadata={}, name='arxiv'),\n",
              "  AIMessage(content='The main author on the \"Retrieval Augmented Generation\" paper is Fuda Ye. Unfortunately, I do not have information on his age.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 908, 'total_tokens': 939, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-753e83ab-4357-4983-bd8b-8164e74f22d4-0', usage_metadata={'input_tokens': 908, 'output_tokens': 31, 'total_tokens': 939})]}"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question = \"Who is the main author on the Retrieval Augmented Generation paper - and how old is he?\"\n",
        "\n",
        "inputs = {\"messages\" : [HumanMessage(content=question)]}\n",
        "\n",
        "rag_agent_app.invoke(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQ-AZxmsEVCP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
